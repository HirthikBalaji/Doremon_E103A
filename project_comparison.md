# PROJECT COMPARISON & DIFFERENTIATION
## AI Agent for Company Websites vs Workforce Contribution Monitor

---

## üìä DETAILED DIFFERENTIATION TABLE

| **Dimension** | **AI Agent for Company Websites** | **Workforce Contribution Monitor** |
|---|---|---|
| **Primary Problem** | Poor website navigation, information discovery, task completion friction | Invisible contribution, activity bias, undetected high performers |
| **User Base** | End-users, first-time visitors, customers, students | Managers, team leads, HR professionals, executives |
| **Core Complexity** | Website structure comprehension, NLP interpretation, browser automation | Data aggregation, metric design, bias elimination, scoring stability |
| **Data Sources** | Website content, site structure, page hierarchy, user interactions | GitHub, Slack, Jira, emails, calendar, code reviews, commit history |
| **AI Challenge** | Understanding ambiguous user intent, navigating unfamiliar UI, handling edge cases | Filtering noise from signal, avoiding metric gaming, ensuring score stability |
| **Output Type** | Step-by-step guidance OR autonomous navigation OR information retrieval | Dashboard, scorecards, team rankings, alert notifications |
| **Real-time Requirement** | NOT critical (users can wait 2-3 seconds) | CRITICAL (managers need live updates for weekly standups) |
| **Scalability Concern** | Website complexity (more pages = harder problem) | Team size (3 people vs 50 people breaks scoring) |
| **Key Success Metric** | User task completion rate, time-to-goal, user satisfaction | Silent Architect detection, score consistency, manager trust |
| **Technology Stack** | LLM + RAG + Web Automation + NLP | ML + Multi-source ETL + Scoring Engine + BI Tools |
| **Ethical Risk** | Low (helps users, reduces friction) | HIGH (impacts performance reviews, career advancement, salary) |
| **Performance Impact** | Indirect (improves UX, retention) | DIRECT (affects compensation, promotions, psychological safety) |
| **Buildability Timeline** | 3-4 months (MVP) | 6-8 months (production-grade) |
| **Cost Complexity** | API costs (GPT/Claude/Open Source LLM) | Data pipeline, ML training, legal/HR review cycles |

---

## üéØ CORE OUTCOME COMPARISON

### **Project 1: AI Agent for Company Websites**
**What Success Looks Like:**
- User arrives at university portal ‚Üí searches "Where do I submit my assignment?" ‚Üí Agent provides 3-step guided path ‚Üí Task completed in <2 minutes (previously: 10+ minutes of frustrated clicking)
- Bank customer ‚Üí "How do I increase my credit limit?" ‚Üí Agent guides through form filling ‚Üí Loan application submitted without support call
- E-commerce site ‚Üí First-time buyer ‚Üí "Where's the order tracking?" ‚Üí Agent shows exact location + real-time order status
- Enterprise employee ‚Üí "What's the onboarding checklist?" ‚Üí Agent walks through 7-step process with links to each resource

**Proof of Success:**
- 70%+ task completion rate for first-time users
- Average task time reduced by 65%
- Support ticket volume down by 40%
- User session duration increases (users find what they need)

---

### **Project 2: Workforce Contribution Monitor**
**What Success Looks Like:**
- **The Silent Architect Gets Noticed:** Sarah solved 3 critical bugs affecting core infrastructure. Her GitHub commits are low (bug fixes are surgical), Slack messages are minimal (she works heads-down). Traditional metrics: 25th percentile. Contribution Monitor shows: 92nd percentile (code complexity score + impact on system stability). Manager sees her during review cycle and allocates promotion budget accordingly.
- **The Loud Router Gets Context:** Mike has 150 commits/month, 500 Slack messages, always in meetings. Traditional view: top performer. Monitor shows: 40% of commits are merge conflicts resolutions (low value), Slack activity is status updates and redirects (adding coordination tax), meeting time = context switching. Score: 45th percentile. Manager provides coaching.
- **Team Health Alert:** Team velocity looks flat. Monitor reveals: 2 members have 60% of commits but working in silos; code review turnaround time is 48 hours blocking 6 others. Manager implements pair programming; velocity improves 35% in 2 weeks.

**Proof of Success:**
- Blind ranking (AI) correlates 85%+ with promotion outcomes (manual assessment)
- Silent Architects have 90%+ retention vs 70% when using commit-count metrics
- Compensation fairness gap closes by 60%
- Team size scales from 3‚Üí50 without metric degradation (Gini coefficient stays <0.3)

---

## üîÑ TECHNICAL ARCHITECTURE COMPARISON

### **Project 1: AI Agent Pipeline**
```
Website Content ‚Üí Scraper/Crawler ‚Üí Knowledge Base (Vector DB)
                                           ‚Üì
                                   RAG System (Embeddings)
                                           ‚Üì
                  User Query ‚Üí NLP Intent Parser ‚Üí Multi-turn LLM
                                           ‚Üì
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚Üì                                                      ‚Üì
         Information Retrieval                          Autonomous Navigation
         (Read-only mode)                              (Browser Automation)
              ‚Üì                                                ‚Üì
        Return Formatted                           Execute Actions & Report
        Answer with Links                          (Click, Form Fill, Submit)
```

### **Project 2: Contribution Monitor Pipeline**
```
GitHub API ‚Üí Commit Analysis ‚Üí Impact Scorer (Code Complexity, Review Time, Merge Conflicts)
                                       ‚Üì
Slack Export ‚Üí NLP Analysis ‚Üí Activity Classifier (Status, Coordination, Problem-solving)
                                       ‚Üì
Jira/Linear ‚Üí Issue Tracking ‚Üí Velocity & Complexity Normalizer
                                       ‚Üì
Calendar Data ‚Üí Meeting Analysis ‚Üí Context Switching Detector
                                       ‚Üì
                    ML Scoring Engine (Weighted Composite)
                                       ‚Üì
              Normalization Layer (Team-Relative Ranking)
                                       ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì                          ‚Üì                       ‚Üì
Manager Dashboard      Alert System           Trend Analysis
(Scorecard View)     (Anomaly Detection)    (Coaching Insights)
```

---

## üí° IMPLEMENTATION COMPLEXITY BREAKDOWN

### **Project 1: Complexity Sources**
1. **Data Acquisition** - Scraping dynamic websites, handling JavaScript-heavy sites
2. **Semantic Understanding** - Ambiguous queries ("Where is X?" could mean 10 different things)
3. **Multi-turn Conversation** - User clarifications, fallback strategies
4. **Browser Automation** - Handling varied UI patterns, login flows, dynamic content
5. **Accuracy** - Outdated information on website = wrong guidance to user

### **Project 2: Complexity Sources**
1. **Data Integration** - Connecting GitHub, Slack, Jira, calendar (OAuth chaos)
2. **Metric Design** - Defining "value" without gaming, avoiding false positives
3. **Normalization** - Making scores fair across 3-person and 50-person teams
4. **Real-time Processing** - Streaming commit data, async Slack exports, time-zone handling
5. **Psychological Safety** - Transparent scoring, explainable decisions, appeal process
6. **Scale Testing** - Metric stability under different team dynamics

---

## üé® USER EXPERIENCE COMPARISON

### **Project 1: End-User Experience**
```
User Types:
‚îú‚îÄ‚îÄ First-time visitors (need guidance)
‚îú‚îÄ‚îÄ Returning users (know layout, need specific help)
‚îú‚îÄ‚îÄ Power users (want shortcuts)
‚îî‚îÄ‚îÄ Accessibility users (need alternative navigation)

Interaction Modes:
‚îú‚îÄ‚îÄ Chat-based Q&A ("Where's my assignment?")
‚îú‚îÄ‚îÄ Guided navigation ("Follow these 5 steps")
‚îú‚îÄ‚îÄ Form completion ("Help me fill the loan application")
‚îî‚îÄ‚îÄ Search-like discovery ("Show me all options for X")
```

### **Project 2: Manager Experience**
```
User Types:
‚îú‚îÄ‚îÄ Team leads (own team view, 5-10 people)
‚îú‚îÄ‚îÄ Directors (multiple teams, 30-50 people)
‚îú‚îÄ‚îÄ HR/Compensation committees (fairness audit)
‚îî‚îÄ‚îÄ Executives (portfolio view, trend tracking)

Interaction Modes:
‚îú‚îÄ‚îÄ Scorecard dashboard (at-a-glance rankings)
‚îú‚îÄ‚îÄ Deep dives (Why is Sarah at 92nd percentile?)
‚îú‚îÄ‚îÄ Peer comparison (Team member A vs Team member B)
‚îú‚îÄ‚îÄ Trend analysis (Individual trajectory over 6 months)
‚îú‚îÄ‚îÄ Alert system (Sarah is overloaded - 70% in emergency PRs)
‚îî‚îÄ‚îÄ Coaching recommendations (Suggested actions per person)
```

---

## üìà SCALABILITY & ROBUSTNESS

### **Project 1: Scaling Challenges**
| **Scale Point** | **Challenge** | **Solution** |
|---|---|---|
| 100 websites | Each site has different structure, nav patterns | Template-based crawlers + site-specific configs |
| 10M+ queries/month | LLM API costs balloon | Implement caching, local smaller models, RAG optimization |
| Real-time changes | Website content updates | Event-driven crawlers, incremental knowledge base updates |
| Dynamic content | JavaScript-rendered pages | Headless browser (Puppeteer) + extended scraping |

### **Project 2: Scaling Challenges**
| **Scale Point** | **Challenge** | **Solution** |
|---|---|---|
| Team: 3‚Üí50 | Metric breaks (high variance in scores) | Scoring formula must scale (Gini coefficient test) |
| Time scale: 1 month‚Üí1 year | Seasonal patterns, project-based bias | Sliding window metrics, context-aware weighting |
| Data volume: 100k‚Üí10M events/day | Pipeline latency increases | Stream processing (Kafka), async ETL, time-series DB |
| Multiple orgs | Scores must stay relative, not absolute | Team-specific baselines, organizational isolation |
| Tool expansion | New Slack workspace, GitHub org, Jira instance | Pluggable connectors, standardized event schema |

---

## üîê ETHICAL & GOVERNANCE CONSIDERATIONS

### **Project 1: Ethical Concerns** (Low Risk)
- **Misinformation Risk**: Agent provides outdated/wrong information ‚Üí User frustration but no permanent harm
- **Privacy**: No PII collection beyond session interactions
- **Bias**: Agent might treat accessibility users differently
- **Solution**: Clear disclaimers ("I may be wrong, verify important info"), human fallback, regular testing with diverse users

### **Project 2: Ethical Concerns** (HIGH Risk)
- **Career Impact**: Wrong score ‚Üí promotion denial, salary stagnation, layoff decisions
- **Mental Health**: Developers see they're ranked 5th out of 50 ‚Üí psychological impact, quiet quitting
- **Gaming**: Developers learn metrics ‚Üí optimize for metrics, not actual impact (commit-bombing, meeting spam)
- **Algorithmic Bias**: If training data from homogeneous team, new hires penalized
- **Transparency**: Manager uses "black box" score to justify decisions ‚Üí legal liability
- **Solution**: 
  - Explainability (show exactly why Alice scored 85/100)
  - Appeals process (30-day window for corrections)
  - Public formula (every team sees exact calculation)
  - Multiple metrics (no single-metric ranking)
  - Regular audits (bias testing quarterly)
  - Opt-in pilot phase (voluntary measurement before enforcement)

---

## üí∞ BUSINESS MODEL & ROI

### **Project 1: Revenue/ROI**
```
B2B SaaS Model:
‚îú‚îÄ‚îÄ Enterprise sites (banks, universities, gov): $500-5000/month
‚îú‚îÄ‚îÄ E-commerce platforms: $200-1000/month
‚îú‚îÄ‚îÄ Self-serve (small websites): $50-200/month
‚îî‚îÄ‚îÄ API access: $0.001-0.01 per query

ROI for customers:
‚îú‚îÄ‚îÄ Support cost savings: 40-60% (fewer tickets)
‚îú‚îÄ‚îÄ User retention: +15-25% (better UX)
‚îú‚îÄ‚îÄ Conversion lift: +8-12% (form completion rates)
‚îî‚îÄ‚îÄ Customer satisfaction: +30% (CSAT scores)

Payback period: 2-3 months
```

### **Project 2: Revenue/ROI**
```
B2B SaaS Model:
‚îú‚îÄ‚îÄ Startup (5-50 people): $300-1000/month
‚îú‚îÄ‚îÄ Growth stage (50-500 people): $2000-10k/month
‚îú‚îÄ‚îÄ Enterprise (500+ people): $20k-100k+/month
‚îî‚îÄ‚îÄ Data licensing (anonymized insights): Ad-hoc deals

ROI for customers:
‚îú‚îÄ‚îÄ Talent acquisition cost savings: 20-30% (better internal promotions)
‚îú‚îÄ‚îÄ Retention lift: +10-15% (fairness perception)
‚îú‚îÄ‚îÄ Performance improvement: +8-12% (clear feedback)
‚îú‚îÄ‚îÄ Compensation fairness: Reduces litigation risk
‚îî‚îÄ‚îÄ Manager confidence: 85%+ trust in scoring

Payback period: 3-4 months (larger orgs), 6+ months (startups)
```

---

# üöÄ PROJECT 1: AI AGENT FOR COMPANY WEBSITES
## 20 BONUS FEATURES TO ADD

### **1. Multi-language Support & Regional Localization**
Auto-detect user language, provide guidance in 15+ languages, handle regional variants (US English vs UK English), currency/unit conversions (INR vs USD for banking sites)

### **2. Voice Interface Integration**
Voice-enabled queries ("Alexa, where's my assignment?"), text-to-speech responses for accessibility, voice command navigation, support for accent variations

### **3. Mobile-First Guidance**
Mobile-specific UI flows (smaller screens require different navigation), progressive disclosure (prioritize mobile-friendly steps), mobile gesture recognition (swipes, long-press)

### **4. Personalized Learning Profile**
Remember user's past searches, surface frequently-asked patterns, adapt guidance complexity to user expertise level, suggest related tasks user might need

### **5. Integration with Popular Automation Tools**
Connect to IFTTT/Zapier, auto-fill forms from external apps, sync data across multiple platforms, calendar/reminder integration after task completion

### **6. Video Tutorial Generation**
Auto-generate screen-record tutorial videos for complex tasks, interactive demos with pause/rewind, embedding video snippets in chat responses

### **7. Feedback Loop & Continuous Improvement**
Thumbs up/down on guidance quality, NPS survey on agent effectiveness, A/B test different explanation styles, community contribution (users suggest better paths)

### **8. Advanced Session Analytics**
Track which guidance paths work best, identify "stuck" users (clicking same area 3+ times), heat maps showing where users abandon, optimization recommendations to website owners

### **9. Predictive Help Suggestions**
Anticipate next user question ("You just searched for assignments; you might want to know about submission deadlines"), proactive alerts (assignment due in 2 hours)

### **10. Offline Capability**
Cache popular Q&A locally, work with poor connectivity, sync when back online, progressive web app for website agents

### **11. Compliance & Regulation Awareness**
For banking/gov sites: GDPR-compliant guidance, ensure no sensitive data in transcripts, audit trails for regulatory inspection, PCI-DSS compliant data handling

### **12. Multi-agent Collaboration**
When single agent can't solve, escalate to human support with full context, agent conference (multiple agents debating best approach for complex queries)

### **13. Integration with Payment Systems**
Guide users through checkout, handle payment failures, refund inquiries, subscription management, multi-currency transactions

### **14. Accessibility Enhancements Beyond Voice**
Support for screen readers, haptic feedback for mobile, keyboard-only navigation, WCAG AAA compliance testing, caption all video content

### **15. Emotion & Frustration Detection**
Detect frustration in text ("This is impossible!"), escalate to human before user abandons, adjust tone (be more empathetic vs efficient), pause and offer break

### **16. Site-Specific Integrations**
For university portals: calendar sync with academic calendar, assignment submission with cloud storage, grade alerts, registration assistance
For e-commerce: wishlist management, price tracking, inventory notifications, return process guidance

### **17. A/B Testing Dashboard for Website Owners**
Test different agent behaviors, measure impact on task completion, optimize agent tone/style for your specific audience, API to get agent performance metrics

### **18. Knowledge Base Versioning**
Maintain historical website changes, acknowledge when site structure changed, help users transitioning from old site layout, deprecation notices for old features

### **19. Contextual Help Overlays**
Augment website with inline help bubbles (no agent chat needed), one-click explanations for jargon, tooltips for form fields, visual annotations

### **20. Cross-Site Knowledge Synthesis**
When site doesn't have answer (e.g., "What's your refund policy?"), fetch from related sites/documents, cite sources clearly, escalate to human for final confirmation

---

# üéØ PROJECT 2: WORKFORCE CONTRIBUTION MONITOR
## 20 BONUS FEATURES TO ADD

### **1. Skill-Based Decomposition**
Map contributions by skill area (backend vs frontend vs DevOps), identify skill gaps in team, recommend cross-training based on impact scores, succession planning for critical skills

### **2. Time-Zone & Async Work Metrics**
Normalize for time zones (don't penalize night-shift workers), measure async contribution quality (thorough PR reviews > quick approvals), detect timezone-related load imbalance

### **3. Mentorship & Pair Programming Weight**
Credit mentors when mentees succeed, measure code review quality (not just count), pair programming sessions with impact attribution, community contribution (open-source, blogging)

### **4. Burnout & Wellness Indicators**
Flag overloaded contributors (70% of code on one person), detect burnout patterns (declining code quality, increasing stress markers in commit messages), recommend workload rebalancing

### **5. Project-Based Attribution**
Instead of individual-only metrics, show contributions to key projects (Project Alpha: X-person team delivered $2M value), CEO-level project tracking, business impact alignment

### **6. Innovation & Experimentation Scoring**
Credit "failed experiments" that generated learning, measure POC velocity, reward risk-taking (not just safe bets), track emerging tech exploration contributions

### **7. Customer Impact Tracing**
Trace commits to customer issues resolved, bugs fixed affecting Y customers, feature adoption metrics, customer satisfaction impact per engineer

### **8. Cross-Team Collaboration Metrics**
Credit helping other teams (no GitHub commit but Slack impact), measure internal API quality (teams using your code), architecture contributions, tech debt reduction

### **9. Code Quality Beyond Commit Count**
Cyclomatic complexity scoring, refactoring impact (technical debt reduction), test coverage improvements, security fixes, performance optimization metrics

### **10. Communication Effectiveness Scoring**
Quality of PR descriptions (helps future maintainers), documentation contributions, blog posts that saved company support time, internal knowledge sharing

### **11. Decision-Making & Leadership Attribution**
RFCs written and adopted, architectural decisions made, feature scope ownership, strategic planning contributions, initiative leadership

### **12. Relative Peer Feedback Integration**
Optional 360-degree feedback weighted into score, peer recognition moments, external interview feedback (from candidate interactions), customer feedback mentions

### **13. Deployment Confidence & Velocity**
Deploy frequency vs reliability (penalize flaky deployments), rollback rate, lead time from commit‚Üíproduction, hotfix response time, on-call reliability

### **14. Learning & Growth Trajectory**
Track skill progression quarter-over-quarter, certifications earned, conference talks given, course completion impact, contribution breadth (deepens vs broadens)

### **15. Tool & Process Improvement Contributions**
Build CI/CD improvements that save team X hours/week, tooling contributions, process optimizations, technical writing for runbooks, infrastructure improvements

### **16. Competitive Benchmarking**
Compare team metrics against industry (DORA metrics), competitive salary intelligence (prevent attrition from comp disparity), retention correlation analysis

### **17. Scenario-Based Scoring Customization**
Different weights for different role types (Platform vs Feature engineer), startup vs enterprise context, phase-of-product (early stage chaos vs stable scale), custom metric weights per org

### **18. Predictive Attrition Risk**
Machine learning model predicting 6-month attrition likelihood, flags high-flight-risk contributors, suggests retention actions, compensation benchmarking recommendations

### **19. Team Dynamics & Collaboration Network**
Social network analysis of who helps whom, identify isolated contributors (possible silos), measure healthy dependencies vs unhealthy bottlenecks, team cohesion scoring

### **20. Historical Analysis & Trend Forecasting**
Track scores over 12 months (is Sarah's score improving?), forecast Q4 contributor list (who's trending up), identify seasonal patterns (slower in summer), predict promotion readiness

---

## üìä FEATURE PRIORITIZATION MATRIX

### **Project 1 MVP (Months 1-2)**
| Priority | Feature | Effort | Impact |
|---|---|---|---|
| P0 | Core navigation guidance + information retrieval | 6 weeks | 95% |
| P0 | Multi-turn conversation + intent clarification | 3 weeks | 90% |
| P1 | Basic accessibility (screen reader support) | 2 weeks | 70% |
| P1 | Feedback loop & basic analytics | 1 week | 65% |
| P2 | Mobile optimization | 2 weeks | 60% |

### **Project 2 MVP (Months 1-3)**
| Priority | Feature | Effort | Impact |
|---|---|---|---|
| P0 | Multi-source ETL (GitHub, Jira, Slack) | 8 weeks | 95% |
| P0 | Impact scoring formula (with audit trail) | 6 weeks | 90% |
| P0 | Manager dashboard + explainability | 4 weeks | 85% |
| P1 | Basic burnout detection | 2 weeks | 60% |
| P1 | Feedback mechanism + appeals process | 2 weeks | 70% |
| P2 | Cross-team collaboration metrics | 3 weeks | 50% |

---

## üé¨ GO-TO-MARKET STRATEGY COMPARISON

### **Project 1: GTM Strategy**
1. **Initial market**: University portals (low-touch, high student volume)
2. **Expansion**: Banking sector (high compliance, high ROI justification)
3. **Distribution**: Direct sales to website owners, API partnerships (Zendesk, Intercom agents)
4. **Pricing tiers**: Free (5 sites, 100 queries), Pro ($500/month), Enterprise (unlimited)

### **Project 2: GTM Strategy**
1. **Initial market**: Mid-market tech companies (50-200 people, engineering-heavy)
2. **Expansion**: Enterprise (1000+ people, high compensation fairness concerns)
3. **Distribution**: Direct sales to Chief People Officer, demo at engineering conferences
4. **Pricing tiers**: Startup ($300), Growth ($2000), Enterprise (custom)
5. **Compliance packaging**: Legal review for performance management use case

---

## ‚öñÔ∏è COMPETITIVE LANDSCAPE

### **Project 1: Competitors**
- OpenAI Operator (browser automation)
- Google Project Mariner (enterprise navigation)
- MultiOn Agent API (custom automation)
- **Differentiation**: Company-specific agent trained on exact site structure, conversational + navigation hybrid

### **Project 2: Competitors**
- WayDev (code metrics only, doesn't solve activity vs impact)
- Lattice/15Five (engagement surveys, not behavioral data)
- ActivTrak (productivity monitoring, privacy concerns)
- **Differentiation**: True activity‚Üíimpact bridge, multi-source synthesis, explicit Silent Architect detection

---

## üìù CONCLUSION & RECOMMENDATION

| **Factor** | **Project 1** | **Project 2** |
|---|---|---|
| **Market Size** | Larger ($10B+ SaaS market) | Moderate ($2-3B HR tech) |
| **Time to Revenue** | 2-3 months | 4-5 months |
| **Ethical Risk** | Low | HIGH (requires governance) |
| **Technical Novelty** | Medium (existing LLM + RAG) | High (novel scoring problem) |
| **Defensibility** | Medium (APIs commoditizing) | High (proprietary scoring formula) |
| **Engineering Complexity** | Medium | High |
| **Business Complexity** | Low | HIGH (HR/legal alignment) |
| **Repeatability** | High (each site is instance) | Low (each org is unique) |
| **Unit Economics** | Strong | Strong |

**Recommendation for B.Tech Student**: 
- **Project 1** if you want: Faster MVP, clearer success metrics, quicker to market
- **Project 2** if you want: Deeper impact, research opportunity, publishable innovation (IEEE paper potential on scoring stability at scale)

---

*Document generated: January 2026 | Suitable for IEEE project submission or academic competition*